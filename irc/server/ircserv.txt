===========================
		VOCAB
===========================

-A callback is simply a function that you pass as an argument to another function or a system API. When you hand off this function, you're essentially saying, “when this particular event happens or task finishes, please call my function so I can take further action



===========================
		COMMANDS
===========================
to connect, run:
`nc <ip address> <port>`
//Netcat (nc) is a raw TCP/UDP tool that allows direct communication with network services without modifying data. It is often used for port scanning, debugging, and data transfer. Unlike e.g. Telnet, Netcat does not interpret control sequences, making it better for binary data transmission


lsof -p $(pgrep ircserv)  - see open sockets

===========================
		FUNCTIONS
===========================
-htons() 	host to network short - byte reordering, if necessary (just pass the ports to one of these always, it won't hurt)
-htonl() 	host to network long
-ntohs() 	network to host short
-ntohl()	network to host long
-pton()		printable to network


		SYSCALLS:
- getaddrinfo()
- getnameinfo() - inverse of getaddrinfo(3): it converts a socket address to a corresponding host and service, in a protocol-independent manner.
- socket() //use PF_INET instead of AF_INET as the domain, despite both basically being same
- bind () //once there is a socket, you bind it to a port
- connect() -initiate a connection on a socket
- listen()
- accept()
- send - send, sendto, sendmsg - send a message on a socket (basically write() with flags)
- recv - recv, recvfrom, recvmsg - receive a message from a socket (basically read() with flags)
- close() and shutdown()
- getpeername() and gethostname()



===========================
	LIMITS
===========================
-rlimit by kernel with api set/getrlimit()




===========================
		  THEORY BASIC
===========================

-Socket is a way to speak to other programs using standard Unix file descriptors.

-There are Internet sockets, UNIX sockets, X.25 sockets, etc.

-Internet sockets have Stream sockets, Datagram sockets, Raw sockets, etc. 
-Telnet or ssh use Stream sockets
-Stream sockets use Transmission Control Protocol (TCP)
-Datagram sockets use User Datagram Protocol (UDP) and are often connectionless, not needing to maintain a connection 
-It’s way faster to fire-and-forget than it is to keep track of what has arrived safely and make sure it’s in order and all that. If you’re sending chat messages, TCP is great; if you’re sending 40 positional updates per second of the players in the world, maybe it doesn’t matter so much if one or two get dropped, and UDP is a good choice.
-Basically, a packet is wrapped (“encapsulated”) in a header (and rarely a footer) by the first protocol (say, the TFTP -Trivial File Transfer Protocol), then the whole thing (TFTP header included) is encapsulated again by the next protocol (say, UDP), then again by the next (IP), then again by the final protocol on the hardware (physical) layer (say, Ethernet). When another computer receives the packet, the hardware strips the Ethernet header, the kernel strips the IP and UDP headers, the TFTP program strips the TFTP header, and it finally has the data.

-TCP and UDP use two addresses: IP (Internet Protocol), and Port number. Port number is a 16-bit number that’s like the local address for the connection.

-Say you want to have a computer that handles incoming mail AND web services—how do you differentiate between the two on a computer with a single IP address?
Well, different services on the Internet have different well-known port numbers. You can see them all in the Big IANA Port List16 or, if you’re on a Unix box, in your /etc/services file. HTTP (the web) is port 80, telnet is port 23, SMTP is port 25, and so on.
Ports under 1024 are often considered special, and usually require special OS privileges to use. 
-Sockets bind to ports. A port is like a mailbox: It’s a designated location where messages arrive. A socket is like a phone call: Once you pick up (accept a connection), the two parties can talk to each other using that established connection.
So ports are entry points, and sockets are active connections (A UDP socket binds to a port so other devices can send packets to it, but it does not maintain a persistent link.).

-Byte order can be Big Endian (b34f - b3 byte stored first, 4f second), and little endian (opposite). Big is "Network Byte Order" and the little is "Host Byte order"


-Socket interface data types:
	-socket descriptor - an int 
	-struct addrinfo {
		int              ai_flags;     // AI_PASSIVE, AI_CANONNAME, etc.
		int              ai_family;    // AF_INET, AF_INET6, AF_UNSPEC
		int              ai_socktype;  // SOCK_STREAM, SOCK_DGRAM
		int              ai_protocol;  // use 0 for "any"
		size_t           ai_addrlen;   // size of ai_addr in bytes
		struct sockaddr *ai_addr;      // struct sockaddr_in or _in6
		char            *ai_canonname; // full canonical hostname

		struct addrinfo *ai_next;      // linked list, next node
	};
	addrinfo structure is a more recent invention, and is used to prep the socket address structures for subsequent use. It’s also used in host name lookups, and service name lookups. That’ll make more sense later when we get to actual usage, but just know for now that it’s one of the first things you’ll call when making a connection
	You might not usually need to write to these structures; oftentimes, a call to getaddrinfo() to fill out your struct addrinfo for you is all you’ll need. You will, however, have to peer inside these structs to get the values out.
	-struct sockaddr {
		unsigned short    sa_family;    // address family, AF_xxx
		char              sa_data[14];  // 14 bytes of protocol address
	}; 
	sa_family can be a variety of things, but it’ll be AF_INET (IPv4) or AF_INET6 (IPv6) for irc.
	sa_data contains a destination address and port number for the socket. This is rather unwieldy since you don’t want to tediously pack the address in the sa_data by hand.
	To deal with struct sockaddr, programmers created a parallel structure: struct sockaddr_in (“in” for “Internet”) to be used with IPv4.	
	And this is the important bit: a pointer to a struct sockaddr_in can be cast to a pointer to a struct sockaddr and vice-versa. So even though connect() wants a struct sockaddr*, you can still use a struct sockaddr_in and cast it at the last minute!
	-struct sockaddr_in {
		short int          sin_family;  // Address family, AF_INET
		unsigned short int sin_port;    // Port number
		struct in_addr     sin_addr;    // Internet address
		unsigned char      sin_zero[8]; // Same size as struct sockaddr
	}; // (see struct sockaddr_in6 for IPv6)
	
- let’s say you have a struct sockaddr_in ina, and you have an IP address “10.12.110.57” or “2001:db8:63b3:1::3490” that you want to store into it. The function you want to use, inet_pton(), converts an IP address in numbers-and-dots notation into either a struct in_addr or a struct in6_addr depending on whether you specify AF_INET or AF_INET6. (“pton” stands for “presentation to network”—you can call it “printable to network” if that’s easier to remember.)
	struct sockaddr_in sa; // IPv4
	struct sockaddr_in6 sa6; // IPv6

	inet_pton(AF_INET, "10.12.110.57", &(sa.sin_addr)); // IPv4
	inet_pton(AF_INET6, "2001:db8:63b3:1::3490", &(sa6.sin6_addr)); // IPv6



Signals from socket():
"A SIGPIPE signal is raised if a process sends or receives on a broken stream; this causes naive processes, which do not handle the signal, to exit."


===========================
	  THEORY HIGH LEVEL
===========================
examples:
    Create all the necessary classes and methods for the project
    Create a socket and handle the server signals.
    Use the poll() function to check if an event has occurred.
    If the event is a new client, register it.
    If the event is data from a registered client, handle it.
    
The ADAPTIVE Communication Environment (ACE) is a 1993 framework that implements many core patterns for concurrent communication software (used by NASA, Motorola Iridium etc). Mostly adopted by now in modern c++ and functions like epoll(). 

    void logging_server (void) {
		initialize acceptor endpoint
		loop forever {
			wait for events
			handle data events
			handle connection events
		}
	}

To start listening, you make the socket(), bind() it to local address, listen(), and then run accept(). 
"If no pending connections are present on the queue, and the socket
       is not marked as nonblocking, accept() blocks the caller until a
       connection is present.  If the socket is marked nonblocking and no
       pending connections are present on the queue, accept() fails with
       the error EAGAIN or EWOULDBLOCK.
       In order to be notified of incoming connections on a socket, you
       can use select(2), poll(2), or epoll(7).  A readable event will be
       delivered when a new connection is attempted and you may then call
       accept() to get a socket for that connection."

GPT about move semantics:
When your class owns a resource (like memory, a file descriptor, a socket…), 
copying that class usually requires a deep copy of the resource. But sometimes, 
you don’t want to copy, you want to transfer the ownership, like in this case:
```std::vector<Socket> vec;
vec.push_back(Socket()); // Here, a temporary Socket is created and moved into the vector```


-cornerstone of nonblocking I/O via multiplexing is that a single socket via poll system allows notifying when it's ready for writing and reading and those are mutually nonexclusive - they can happen asynchronously.
-So we send() when the socket is ready for the POLLOUT operation, and there is data in the send buffer to be sent.
-We recv() when there is a POLLIN event from the socket, but because a recv() call may not give us the full command at once (irc line, ending in "\r\n"), we need to accumulate data and append any received bytes into the readBuffer. Once the end of the command is detected, parse it - and carefully leave any incomplete message in the buffer for the next read. 























https://www.scottklement.com/rpg/socktut/nonblocking.html:
```
6.5. Blocking vs. non-blocking sockets

So far in this chapter, you've seen that select() can be used to detect when data is available to read from a socket. However, there are times when its useful to be able to call send(), recv(), connect(), accept(), etc without having to wait for the result.

For example, let's say that you're writing a web browser. You try to connect to a web server, but the server isn't responding. When a user presses (or clicks) a stop button, you want the connect() API to stop trying to connect.

With what you've learned so far, that can't be done. When you issue a call to connect(), your program doesn't regain control until either the connection is made, or an error occurs.

The solution to this problem is called "non-blocking sockets".

By default, TCP sockets are in "blocking" mode. For example, when you call recv() to read from a stream, control isn't returned to your program until at least one byte of data is read from the remote site. This process of waiting for data to appear is referred to as "blocking". The same is true for the write() API, the connect() API, etc. When you run them, the connection "blocks" until the operation is complete.

Its possible to set a descriptor so that it is placed in "non-blocking" mode. When placed in non-blocking mode, you never wait for an operation to complete. This is an invaluable tool if you need to switch between many different connected sockets, and want to ensure that none of them cause the program to "lock up."

If you call "recv()" in non-blocking mode, it will return any data that the system has in it's read buffer for that socket. But, it won't wait for that data. If the read buffer is empty, the system will return from recv() immediately saying ``"Operation Would Block!"''.

The same is true of the send() API. When you call send(), it puts the data into a buffer, and as it's read by the remote site, it's removed from the buffer. If the buffer ever gets "full", the system will return the error 'Operation Would Block" the next time you try to write to it.

Non-blocking sockets have a similar effect on the accept() API. When you call accept(), and there isn't already a client connecting to you, it will return 'Operation Would Block', to tell you that it can't complete the accept() without waiting...

The connect() API is a little different. If you try to call connect() in non-blocking mode, and the API can't connect instantly, it will return the error code for 'Operation In Progress'. When you call connect() again, later, it may tell you 'Operation Already In Progress' to let you know that it's still trying to connect, or it may give you a successful return code, telling you that the connect has been made.

Going back to the "web browser" example, if you put the socket that was connecting to the web server into non-blocking mode, you could then call connect(), print a message saying "connecting to host www.floofy.com..." then maybe do something else, and them come back to connect() again. If connect() works the second time, you might print "Host contacted, waiting for reply..." and then start calling send() and recv(). If the connect() is still pending, you might check to see if the user has pressed a "abort" button, and if so, call close() to stop trying to connect.

Non-blocking sockets can also be used in conjunction with the select() API. In fact, if you reach a point where you actually WANT to wait for data on a socket that was previously marked as "non-blocking", you could simulate a blocking recv() just by calling select() first, followed by recv().

The "non-blocking" mode is set by changing one of the socket's "flags". The flags are a series of bits, each one representing a different capability of the socket. So, to turn on non-blocking mode requires three steps:

    Call the fcntl() API to retrieve the socket descriptor's current flag settings into a local variable.

    In our local variable, set the O_NONBLOCK (non-blocking) flag on. (being careful, of course, not to tamper with the other flags)

    Call the fcntl() API to set the flags for the descriptor to the value in our local variable.
    ```


-If O_NONBLOCK is set and the STREAM cannot accept data, write() shall return -1 and set errno to [EAGAIN].
-If O_NONBLOCK is set and part of the buffer has been written while a condition in which the STREAM cannot accept additional data occurs, write() shall terminate and return the number of bytes written




- How do I use TCP_NODELAY?
  First off, be sure you really want to use it in the first place.  It
  will disable the Nagle algorithm (see ``2.11 How can I force a socket
  to send the data in its buffer?''), which will cause network traffic
  to increase, with smaller than needed packets wasting bandwidth.
  Also, from what I have been able to tell, the speed increase is very
  small, so you should probably do it without TCP_NODELAY first, and
  only turn it on if there is a problem.

  Here is a code example, with a warning about using it from Andrew
  Gierth:

         int flag = 1;
         int result = setsockopt(sock,            /* socket affected */
                                 IPPROTO_TCP,     /* set option at TCP level */
                                 TCP_NODELAY,     /* name of option */
                                 (char *) &flag,  /* the cast is historical
                                                         cruft */
                                 sizeof(int));    /* length of option value */
         if (result < 0)
            ... handle the error ...

  TCP_NODELAY is for a specific purpose; to disable the Nagle buffering
  algorithm. It should only be set for applications that send frequent
  small bursts of information without getting an immediate response,
  where timely delivery of data is required (the canonical example is
  mouse movements).

[an error occurred while processing this directive]







































================== 
Send/Recv a la Copilot: https://copilot.microsoft.com/shares/MjeewSVRy6k7spppxTpCj
==================

Below is an approach you can adopt that keeps your nonblocking, poll-driven architecture robust, modular, and maintainable:

---

### Use Per-Client Buffers

**Why?**  
Because nonblocking sockets almost always return less-than‑expected amounts of data (or even none) when you try to send or receive messages, each client should maintain two buffers: one for incoming data (a read buffer) and one for outgoing data (a write buffer). This lets you accumulate incoming fragments until you have a complete message (for example, until you detect the IRC-standard “\r\n” line ending), and it lets you queue up outgoing data if a send call only writes part of your intended message.

**How?**  
Each `Client` object might include data members like:

```cpp
class Client {
public:
    int socket;
    std::string readBuffer;  // Stores fragments of inbound commands.
    std::string writeBuffer; // Stores unsent outbound messages.
    // Additional client info (nicknames, state, etc.)
};
```

---

### Reading Data (recv)

1. **Polling for POLLIN:**  
   In your main poll loop, when a client’s file descriptor is flagged with `POLLIN`, you should call `recv()` in a loop until you get `EAGAIN` or `EWOULDBLOCK`. This way, even if there’s more than one message in the socket’s buffer, you won’t miss any.

2. **Accumulate Data:**  
   Append any received bytes into the client’s `readBuffer`. Once you detect a full line (typically ending with “\r\n” in IRC protocol), parse out the command and process it. Be careful to leave any incomplete message in the buffer for the next read.

3. **Handle Disconnects and Errors:**  
   If `recv()` returns 0, the client has disconnected. If it returns -1 with an error other than `EAGAIN` or `EWOULDBLOCK`, then handle the error appropriately (e.g., log the issue and close the connection).

**Example:**

```cpp
void handleClientRead(Client &client) {
    char buf[4096];
    ssize_t bytesRead = recv(client.socket, buf, sizeof(buf), 0);
    if (bytesRead > 0) {
        client.readBuffer.append(buf, bytesRead);
        // Process complete messages (e.g., look for "\r\n")
        size_t pos;
        while ((pos = client.readBuffer.find("\r\n")) != std::string::npos) {
            std::string message = client.readBuffer.substr(0, pos);
            client.readBuffer.erase(0, pos + 2);
            // Process the command/message
        }
    } else if (bytesRead == 0) {
        // Client disconnected gracefully.
        close(client.socket);
        // Remove client from clients map, cleanup, etc.
    } else {
        if (errno != EAGAIN && errno != EWOULDBLOCK) {
            // Log error and close connection if required.
        }
    }
}
```

---

### Writing Data (send)

1. **Queue Outgoing Data:**  
   Whenever you need to send a message (like the welcome message or a broadcast to channel members), append the message to the client’s `writeBuffer`. For instance, immediately after accepting a new client, you might do:

   ```cpp
   client.writeBuffer += "Welcome to IRC, " + client.nickname + "\r\n";
   ```

2. **Watch for POLLOUT:**  
   In your poll loop, if a socket is flagged with `POLLOUT` (or if you know there’s data waiting in the `writeBuffer`), call `send()` to flush as many bytes as possible. Because `send()` may only send part of the data, always adjust (or remove) the sent portion from your buffer.

3. **Handle Partial Sends:**  
   If `send()` returns a value less than the size of your buffer, keep the remaining data. If it returns `-1` with `errno == EAGAIN` or `EWOULDBLOCK`, wait until the poll indicates the socket is writable again.

**Example:**

```cpp
void handleClientWrite(Client &client) {
    if (client.writeBuffer.empty())
        return;

    ssize_t sent = send(client.socket, client.writeBuffer.data(), client.writeBuffer.size(), 0);
    if (sent > 0) {
        client.writeBuffer.erase(0, sent);
    } else if (sent == -1 && errno != EAGAIN && errno != EWOULDBLOCK) {
        // Handle error (log it and close the connection if necessary)
        close(client.socket);
    }
}
```

Don’t forget to update your `pollfd` structure for the client’s socket. If there is data in `writeBuffer`, ensure you’re monitoring for `POLLOUT` events. When the buffer is empty, you can remove `POLLOUT` from the events mask.

---

### Broadcasting Messages to Channel Members

When a message is sent in a channel:

1. **Iterate Over Channel's Clients:**  
   Use your channels vector (or however you track memberships) to iterate through the connected clients.

2. **Queue the Message:**  
   For each client (except maybe the originating client, if needed), append the message to their `writeBuffer`.

3. **Trigger Writing:**  
   Make sure that relevant sockets are added with the `POLLOUT` flag if they already aren’t.

**Pseudo-code Example:**

```cpp
void broadcastToChannel(const Channel &channel, const std::string &message) {
    for (Client* client : channel.clients) {
        client->writeBuffer += message + "\r\n";
        // Ensure that the poll event for this client's socket includes POLLOUT.
    }
}
```

---

### Error Handling and Robustness

- **Nonblocking Semantics:**  
  Always be prepared for a `send()` to return `EAGAIN` or `recv()` to return `EWOULDBLOCK`. Your design with buffers handles this cleanly.

- **Partial Data:**  
  Since partial reads/writes are common, never assume that a complete message will be processed in one call. Gradually build and then parse your buffers.

- **Poll Maintenance:**  
  Update your `pollfd` array after any change in the client’s state (e.g., adding or removing `POLLOUT` when you write to a client or finish a send operation).

- **Resource Cleanup:**  
  Always clean up sockets and remove clients from your maps/vectors when a socket disconnects or encounters an unrecoverable error.

---

### Summing Up

The key to a robust implementation in your ft_irc server is managing state effectively:
- **Per-Client Buffers:** Use these to avoid losing data when partial sends/receives occur.
- **Event-Driven I/O:** Leverage poll’s event notifications to know exactly when to read or write, ensuring nonblocking operations work seamlessly.
- **Graceful Error Handling:** Check return values from `send()` and `recv()`, handle partial transfers, and act on errors in a predictable way.
- **Structured Broadcasts:** When sending channel messages or welcome messages, queue them in the proper output buffers and ensure poll monitors these sockets for writability.

By following this pattern, you’ll find your server both efficient and resilient, capable of handling simultaneous clients and commands gracefully.

int ready = poll(&pollFds_[0], pollFds_.size(), -1);
		if (ready < 0) {
			std::cerr << "poll() error: " << strerror(errno) << std::endl;
			if (errno == EINTR) continue;
			throw std::runtime_error("poll() failed");
		}
		else if (ready == 0) {
			std::cout << "poll() timeout (shouldn't happen with infinite timeout)" << std::endl;
			continue;
		}
		
		// Check all file descript

---

### Further Considerations

- **Advanced I/O Libraries:**  
  If your project scope permits in the future, consider integrating an advanced event-driven library (like Boost.Asio or libuv). These libraries abstract some of these low-level details while still maintaining nonblocking behavior and efficient event management.

- **Protocol Parsing:**  
  As IRC protocols can get complex, ensure your command parser robustly handles edge cases and malformed inputs. Consider state machines for parsing commands that span multiple reads.

- **Performance:**  
  Nonblocking I/O and poll work well, but as your user base grows, you might explore more scalable event models like epoll (on Linux) or kqueue (on BSD/Mac) for further performance improvements.

Now that you have a solid blueprint, what parts of the implementation or design do you see as the most challenging? Would you like to see more code samples or dive deeper into any specific aspect?



#include <poll.h>
#include <sys/socket.h>
#include <unistd.h>
#include <errno.h>
#include <cstring>
#include <iostream>
#include <vector>
#include <map>
#include <string>

// Basic client struct with per-socket buffers.
struct Client {
    int sockfd;
    std::string readBuffer;
    std::string writeBuffer;
    // Additional client-related info could go here.
};

// Global data structure storing clients by socket descriptor.
std::map<int, Client*> clients;

// The pollfd vector that you'll update each iteration.
std::vector<struct pollfd> pollfds;

// Refresh the pollfds array to include each client's current events.
// Notice that if a client's writeBuffer is non-empty, the POLLOUT event is added.
void refreshPollfds() {
    pollfds.clear();
    for (auto& pair : clients) {
        int fd = pair.first;
        Client* client = pair.second;
        struct pollfd pfd;
        pfd.fd = fd;
        pfd.events = POLLIN; // Always want to read.
        if (!client->writeBuffer.empty())
            pfd.events |= POLLOUT; // Add POLLOUT if there's pending outgoing data.
        pfd.revents = 0;
        pollfds.push_back(pfd);
    }
}

// Handle reading from a client socket when POLLIN is set.
void handleClientRead(Client* client) {
    char buf[4096];
    ssize_t bytes = recv(client->sockfd, buf, sizeof(buf), 0);
    if (bytes > 0) {
        // Append to the client's read buffer.
        client->readBuffer.append(buf, bytes);
        // Process complete messages if a full command is received. For IRC, you might use "\r\n".
        size_t pos;
        while ((pos = client->readBuffer.find("\r\n")) != std::string::npos) {
            std::string message = client->readBuffer.substr(0, pos);
            client->readBuffer.erase(0, pos + 2);
            // Process the IRC command/message.
            std::cout << "Received message: " << message << std::endl;
        }
    } else if (bytes == 0) {
        // A return of 0 means the client disconnected.
        std::cout << "Client disconnected (read 0 bytes)" << std::endl;
        close(client->sockfd);
        delete client;
        clients.erase(client->sockfd);
    } else {
        // An error occurred.
        if (errno != EAGAIN && errno != EWOULDBLOCK) {
            std::cerr << "recv error on fd " << client->sockfd
                      << ": " << strerror(errno) << std::endl;
            close(client->sockfd);
            delete client;
            clients.erase(client->sockfd);
        }
    }
}

// Handle writing pending data to a client socket when POLLOUT is set.
void handleClientWrite(Client* client) {
    if (client->writeBuffer.empty())
        return;

    ssize_t sent = send(client->sockfd, client->writeBuffer.data(), client->writeBuffer.size(), 0);
    if (sent > 0) {
        // Remove sent data from the buffer.
        client->writeBuffer.erase(0, sent);
    } else if (sent == -1 && errno != EAGAIN && errno != EWOULDBLOCK) {
        std::cerr << "send error on fd " << client->sockfd
                  << ": " << strerror(errno) << std::endl;
        close(client->sockfd);
        delete client;
        clients.erase(client->sockfd);
    }
}

int main() {
    // Set up your listening socket and accept connections into the `clients` map.
    // For this example, assume clients get added to 'clients' as they connect.

    const int timeout = 1000;  // Timeout in milliseconds.
    while (true) {
        // Update pollfds based on the current clients and their buffer states.
        refreshPollfds();

        int ready = poll(pollfds.data(), pollfds.size(), timeout);
        if (ready < 0) {
            perror("poll");
            break;
        }

        for (auto& pfd : pollfds) {
            // Handle errors. POLLERR, POLLHUP, or POLLNVAL indicate issues.
            if (pfd.revents & (POLLERR | POLLHUP | POLLNVAL)) {
                std::cerr << "Error on socket " << pfd.fd << std::endl;
                if (clients.find(pfd.fd) != clients.end()) {
                    close(pfd.fd);
                    delete clients[pfd.fd];
                    clients.erase(pfd.fd);
                }
                continue;
            }

            // Retrieve the client reference based on the file descriptor.
            auto it = clients.find(pfd.fd);
            if (it == clients.end())
                continue;
            Client* client = it->second;

            // If the socket is flagged as ready for reading, process incoming data.
            if (pfd.revents & POLLIN)
                handleClientRead(client);

            // If the socket is flagged as ready for writing, flush outgoing data.
            if (pfd.revents & POLLOUT)
                handleClientWrite(client);
        }

        // Additional activities may take place here,
        // such as managing new connections, handling timeouts, etc.
    }

    // Cleanup remaining clients.
    for(auto& pair : clients) {
        close(pair.second->sockfd);
        delete pair.second;
    }
    return 0;
}


d has nothing to do with Linux Shell. As part of login procedure, PAM invokes system-auth submodule, which in turn involves 














//in regards to using vector for pollFds:
Considerations

    Valid Index: Ensure that index is less than pollFds_.size(). If index is out-of-range, the behavior is undefined and may lead to a crash or other unpredictable behavior.

    Iterator Invalidation: Erasing an element from a vector invalidates iterators, pointers, and references to the erased element and any elements following it. Be cautious if you’re iterating over the vector or holding onto iterators for later use.

    Performance Implications: Removing an element from somewhere in the middle of a vector will shift subsequent elements to fill the gap, which can be an O(n) operation. If the vector is large and order does not matter, you might consider techniques such as swapping the element to the end and then calling pop_back().
    
    
    
what is the best storage way for pollFds for poll()? at any point in the for loop through the storage, one or more of the fds may be erased from the loop

The most common—and generally best—container to use for storing pollfd structures is a std::vector<pollfd>. This is because:

    Contiguity: poll() requires a pointer to a contiguous block of pollfd structures, and std::vector guarantees that its storage is contiguous.

    Flexibility for Erasure: You mentioned that one or more file descriptors might be erased during the loop. Although erasing elements from a vector can be an O(n) operation (and it invalidates iterators or indices for the elements that follow), this is usually acceptable given that the typical number of sockets isn't enormous.

    To safely remove elements while iterating over a vector:

        Iterate Backwards: Loop in reverse order so that removing an element doesn’t shift later elements that you have yet to process.

        Index Management: If iterating forwards, when you erase an element, do not increment the index afterward since the next element shifts into the current index.

Example: Iterating Forward with Index Adjustment
for (size_t i = 0; i < pollFds_.size(); /* no increment here */) {
    if (/* condition to remove pollFds_[i] */) {
        pollFds_.erase(pollFds_.begin() + i);
        // Don't increment i here, because the next element
        // now occupies index i.
    } else {
        ++i;
    }
}
Example: Iterating Backwards
for (size_t i = pollFds_.size(); i-- > 0; ) {  // Using reverse index iteration
    if (/* condition to remove pollFds_[i] */) {
        pollFds_.erase(pollFds_.begin() + i);
    }
}

    Alternative Approaches: For scenarios where many erasures occur and performance is a concern, you could consider:

        Deferred Erasure: Mark file descriptors for removal during the event loop and erase them in one go after processing the poll events.

        Using std::remove_if: This algorithm can help remove multiple elements in one pass, although you'll need to adjust accordingly since you'll then have to call .erase() on the returned iterator.

    However, because poll() requires a contiguous block, you’ll likely need to convert the container’s state back to a contiguous array. std::vector is still ideal for this purpose.

Summary

    std::vector<pollfd> is usually the best choice because of its contiguous storage, which is required by poll().

    When erasing elements from the vector, consider iterating in reverse or carefully managing your index to avoid skipping elements.

    If performance is an issue due to frequent erasures, you can apply deferred deletion strategies.

This design is widely used in event-driven server implementations, ensuring both efficiency and correctness when monitoring file descriptors with poll().

Would you like to look into further refinements, such as managing deferred deletions or other container alternatives?
